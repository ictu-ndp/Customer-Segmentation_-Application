# -*- coding: utf-8 -*-
"""SIC_data preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DOrjRMPUnwbRg573GoT_mcGksE3WwCE

#C√†i ƒë·∫∑t Pyspark
"""

"""# Input th∆∞ vi·ªán v√† ƒë·ªçc d·ªØ li·ªáu"""

import pyspark
import pandas as pd
import numpy as np

from pyspark.sql import SparkSession


<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <title>ƒê·∫øm ng∆∞·ª£c ƒë·∫øn ng√†y b·∫°n ch·ªçn</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #f0f2f5;
      text-align: center;
      padding-top: 50px;
    }
    h1 {
      color: #333;
    }
    input, button {
      padding: 10px;
      font-size: 16px;
      margin: 5px;
    }
    #countdown {
      margin-top: 30px;
      font-size: 24px;
      color: #007b00;
      font-weight: bold;
    }
    #error {
      color: red;
      font-size: 16px;
    }
  </style>
</head>
<body>

  <h1>üïí ƒê·∫øm ng∆∞·ª£c ƒë·∫øn th·ªùi ƒëi·ªÉm b·∫°n ch·ªçn</h1>
  <input type="datetime-local" id="targetDateTime">
  <br>
  <button onclick="startCountdown()">B·∫Øt ƒë·∫ßu</button>
  <button onclick="resetCountdown()">Reset</button>

  <div id="error"></div>
  <div id="countdown"></div>

  <script>
    let interval;

    function startCountdown() {
      clearInterval(interval);
      const input = document.getElementById("targetDateTime").value;
      const countdown = document.getElementById("countdown");
      const error = document.getElementById("error");

      if (!input) {
        error.innerText = "üõë Vui l√≤ng ch·ªçn ng√†y gi·ªù!";
        countdown.innerText = "";
        return;
      }

      const target = new Date(input);

      interval = setInterval(() => {
        const now = new Date();
        const diff = target - now;

        if (diff <= 0) {
          clearInterval(interval);
          countdown.innerText = "üéâ ƒê√£ ƒë·∫øn th·ªùi ƒëi·ªÉm b·∫°n ch·ªçn!";
          return;
        }

        const days = Math.floor(diff / (1000 * 60 * 60 * 24));
        const hours = Math.floor((diff / (1000 * 60 * 60)) % 24);
        const minutes = Math.floor((diff / (1000 * 60)) % 60);
        const seconds = Math.floor((diff / 1000) % 60);

        countdown.innerText = `‚è≥ C√≤n l·∫°i: ${days} ng√†y, ${hours} gi·ªù, ${minutes} ph√∫t, ${seconds} gi√¢y.`;
        error.innerText = "";
      }, 1000);
    }

    function resetCountdown() {
      clearInterval(interval);
      document.getElementById("targetDateTime").value = "";
      document.getElementById("countdown").innerText = "";
      document.getElementById("error").innerText = "";
    }
  </script>
</body>
</html>







from datetime import datetime

def tinh_thoi_gian_con_lai(ngay_tuong_lai_str):
    try:
        hien_tai = datetime.now()
        ngay_tuong_lai = datetime.strptime(ngay_tuong_lai_str, '%Y-%m-%d %H:%M:%S')
        khoang_thoi_gian = ngay_tuong_lai - hien_tai

        if khoang_thoi_gian.total_seconds() < 0:
            return "üõë Ng√†y b·∫°n nh·∫≠p ƒë√£ n·∫±m trong qu√° kh·ª©."

        so_ngay = khoang_thoi_gian.days
        tong_giay_con_lai = khoang_thoi_gian.seconds
        so_gio = tong_giay_con_lai // 3600
        so_phut = (tong_giay_con_lai % 3600) // 60
        so_giay = tong_giay_con_lai % 60

        return (so_ngay, so_gio, so_phut, so_giay)

    except ValueError:
        return "üõë ƒê·ªãnh d·∫°ng ng√†y gi·ªù kh√¥ng h·ª£p l·ªá. H√£y nh·∫≠p theo ƒë·ªãnh d·∫°ng: YYYY-MM-DD HH:MM:SS"

ngay_nhap = "2025-12-31 23:59:59"
ket_qua = tinh_thoi_gian_con_lai(ngay_nhap)

if isinstance(ket_qua, tuple):
    print(f"‚è≥ C√≤n l·∫°i: {ket_qua[0]} ng√†y, {ket_qua[1]} gi·ªù, {ket_qua[2]} ph√∫t, {ket_qua[3]} gi√¢y.")
else:
    print(ket_qua)








# Kh·ªüi t·∫°o SparkSession v·ªõi c·∫•u h√¨nh HDFS
spark = SparkSession.builder \
    .appName("Customer Preprocessing") \
    .master("local[*]") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .config("spark.hadoop.fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem") \
    .getOrCreate()

# ƒê·ªçc CSV t·ª´ HDFS
hdfs_path = "hdfs://localhost:9000/user/iamphuong/customer/customer_data1.csv"
df_spark = spark.read.csv(hdfs_path, header=True, inferSchema=True)

# Hi·ªÉn th·ªã m·ªôt v√†i d√≤ng d·ªØ li·ªáu
df_spark.show()

print(spark.version)

# Hi·ªÉn th·ªã d·ªØ li·ªáu
df_spark.show()

df_spark.printSchema()

df_spark.describe().show()

print(df_spark.columns)

print(df_spark.dtypes)

"""#Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"""

# X√≥a t·∫•t c·∫£ c√°c gi√° tr·ªã NULL
df_spark = df_spark.dropna()

#Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt cho vi·ªác ph√¢n t√≠ch v√† x·ª≠ l√Ω
df_spark = df_spark.drop("shopping_mall")

#Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
from pyspark.sql.functions import col

df_spark = df_spark.withColumn("age", col("age").cast("integer"))
df_spark = df_spark.withColumn("quantity", col("quantity").cast("integer"))
df_spark = df_spark.withColumn("price", col("price").cast("double"))

#Thay ƒë·ªïi ƒë·ªãnh d·∫°ng ng√†y th√°ng
from pyspark.sql.functions import to_date

df_spark = df_spark.withColumn("invoice_date", to_date(col("invoice_date"), "yyyy-MM-dd"))

#T·∫°o th√™m 1 tr∆∞·ªùng th√¥ng tin m·ªõi "T·ªïng ti·ªÅn c·ªßa m·ªói h√≥a ƒë∆°n"
df_spark = df_spark.withColumn("total_amount", col("quantity") * col("price"))

#X√≥a c√°c gi√° tr·ªã tr√πng l·∫∑p
df_spark = df_spark.dropDuplicates()

#M√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCols=["gender", "category", "payment_method"],
                        outputCols=["gender_indexed", "category_indexed", "payment_method_indexed"])
df_spark = indexer.fit(df_spark).transform(df_spark)

# T·∫°o c·ªôt "T·ªïng ti·ªÅn c·ªßa m·ªói h√≥a ƒë∆°n"
df_spark = df_spark.withColumn("total_amount", col("quantity") * col("price"))

#Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë Scaling
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["age", "quantity", "price", "total_amount"],
                            outputCol="features")
df_spark = assembler.transform(df_spark)

scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
df_spark = scaler.fit(df_spark).transform(df_spark)

#T√≠nh to√°n th·ªëng k√™ c∆° b·∫£n
df_spark.describe().show()

# X√≥a c·ªôt 'features' trong DataFrame
if 'features' in df_spark.columns:
    df_spark = df_spark.drop('features')

from pyspark.sql import SparkSession
from pyspark.sql.functions import datediff, max, count, sum, col, when, lit, expr
from pyspark.sql.window import Window

# Chuy·ªÉn ƒë·ªïi invoice_date th√†nh ki·ªÉu date
df_spark = df_spark.withColumn("invoice_date", col("invoice_date").cast("date"))

# T√≠nh Recency, Frequency, v√† Monetary
max_date = df_spark.agg(max("invoice_date")).collect()[0][0]
df_rfm = df_spark.groupBy("customer_id").agg(
    datediff(lit(max_date), max("invoice_date")).alias("recency"),
    count("invoice_no").alias("frequency"),
    sum(col("price") * col("quantity")).alias("monetary")
)

from pyspark.ml.feature import VectorAssembler

# T·∫°o VectorAssembler ƒë·ªÉ k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng th√†nh m·ªôt vector
assembler = VectorAssembler(inputCols=["scaled_features"], outputCol="features")

# Chuy·ªÉn ƒë·ªïi DataFrame v·ªõi VectorAssembler
df_spark = assembler.transform(df_spark)

#Hi·ªÉn th·ªã d·ªØ li·ªáu
df_spark.show()

"""#Tr·ª±c quan h√≥a d·ªØ li·ªáu"""

# Chuy·ªÉn ƒë·ªïi sang Pandas DataFrame
df_pandas = df_spark.select("*").toPandas()

#Bi·ªÅu ƒë·ªì ph√¢n b·ªë ƒë·ªô tu·ªïi c·ªßa kh√°ch h√†ng
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df_pandas['age'], bins=30, kde=True)
plt.title('Age Distribution of Customers')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

#Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t c·ªßa ph∆∞∆°ng th·ª©c thanh to√°n

plt.figure(figsize=(10, 6))
sns.countplot(x='payment_method', data=df_pandas)
plt.title('Payment Method Frequency')
plt.xlabel('Payment Method')
plt.ylabel('Frequency')
plt.show()

#Bi·ªÉu ƒë·ªì t∆∞∆°ng quan gi·ªØa s·ªë l∆∞·ª£ng v√† t·ªïng ti·ªÅn m·ªói h√≥a ƒë∆°n

plt.figure(figsize=(10, 6))
sns.scatterplot(x='quantity', y='total_amount', data=df_pandas)
plt.title('Quantity vs Total Amount')
plt.xlabel('Quantity')
plt.ylabel('Total Amount')
plt.show()

"""#Ph√¢n chia t·∫≠p d·ªØ li·ªáu"""

train_df, temp_df = df_spark.randomSplit([0.7, 0.3], seed=42)
validation_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)

"""#X√¢y d·ª±ng m√¥ h√¨nh h·ªçc m√°y

###K-Means
"""

from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.feature import VectorAssembler

# T·∫°o VectorAssembler cho c√°c ƒë·∫∑c tr∆∞ng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="features")
df_rfm = assembler.transform(df_rfm)

# X√¢y d·ª±ng m√¥ h√¨nh K-Means
kmeans = KMeans(k=3, seed=1)
model = kmeans.fit(df_rfm)

# D·ª± ƒëo√°n c·ª•m cho t·ª´ng kh√°ch h√†ng
predictions_km = model.transform(df_rfm)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
predictions_km.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Th√™m t√™n c·ª•m v√†o k·∫øt qu·∫£
def label_cluster(prediction):
    if prediction == 0:
        return "Kh√°ch h√†ng kh√¥ng ti·ªÅm nƒÉng"
    elif prediction == 1:
        return "Kh√°ch h√†ng ti·ªÅm nƒÉng"
    else:
        return "Kh√°ch h√†ng m·ªõi"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

label_udf = udf(label_cluster, StringType())
predictions_1 = predictions_km.withColumn("customer_segment", label_udf("prediction"))

# Hi·ªÉn th·ªã k·∫øt qu·∫£
predictions_1.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

from pyspark.sql import functions as F

# D·ª± ƒëo√°n c·ª•m cho t·ª´ng kh√°ch h√†ng
predictions = model.transform(df_rfm)

# Th√™m t√™n c·ª•m v√†o k·∫øt qu·∫£
predictions = predictions_1.withColumn("customer_segment", label_udf("prediction"))

# S·ª≠ d·ª•ng groupBy ƒë·ªÉ t√≠nh to√°n th·ªëng k√™ cho t·ª´ng c·ª•m
grouped_results_km = predictions.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
grouped_results_km.show()

# ƒê√°nh gi√° m√¥ h√¨nh
evaluator = ClusteringEvaluator()
silhouette = evaluator.evaluate(predictions)
print(f'Silhouette Score: {silhouette}')

"""###Bisecting Kmeans"""

from pyspark.sql.functions import col

# X√≥a c·ªôt 'features' n·∫øu n√≥ ƒë√£ t·ªìn t·∫°i
if 'features' in df_rfm.columns:
    df_rfm = df_rfm.drop('features')

# T·∫°o VectorAssembler cho c√°c ƒë·∫∑c tr∆∞ng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="features")
df_rfm = assembler.transform(df_rfm)

# X√¢y d·ª±ng m√¥ h√¨nh BisectingKMeans
from pyspark.ml.clustering import BisectingKMeans

bkm = BisectingKMeans(k=3, seed=1)
model = bkm.fit(df_rfm)

# D·ª± ƒëo√°n c·ª•m cho t·ª´ng kh√°ch h√†ng
predictions_bkm = model.transform(df_rfm)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
predictions_bkm.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Th√™m t√™n c·ª•m v√†o k·∫øt qu·∫£
def label_cluster(prediction):
    if prediction == 0:
        return "Kh√°ch h√†ng kh√¥ng ti·ªÅm nƒÉng"
    elif prediction == 1:
        return "Kh√°ch h√†ng ti·ªÅm nƒÉng"
    else:
        return "Kh√°ch h√†ng m·ªõi"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

label_udf = udf(label_cluster, StringType())
predictions_2 = predictions_bkm.withColumn("customer_segment", label_udf("prediction"))

# Hi·ªÉn th·ªã k·∫øt qu·∫£
predictions_2.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

from pyspark.sql import functions as F

# S·ª≠ d·ª•ng groupBy ƒë·ªÉ t√≠nh to√°n th·ªëng k√™ cho t·ª´ng c·ª•m
grouped_results_bkm = predictions_2.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
grouped_results_bkm.show()



import tkinter as tk
from tkinter import ttk
from tkcalendar import DateEntry
from datetime import datetime
import time

def tinh_thoi_gian_con_lai():
    ngay = calendar.get_date().strftime('%Y-%m-%d')
    gio = gio_entry.get()
    phut = phut_entry.get()
    giay = giay_entry.get()

    try:
        thoi_gian_str = f"{ngay} {gio}:{phut}:{giay}"
        ngay_tuong_lai = datetime.strptime(thoi_gian_str, '%Y-%m-%d %H:%M:%S')
        hien_tai = datetime.now()
        khoang_thoi_gian = ngay_tuong_lai - hien_tai

        if khoang_thoi_gian.total_seconds() < 0:
            ket_qua_label.config(text="üõë Ng√†y gi·ªù ƒë√£ n·∫±m trong qu√° kh·ª©.")
            return

        so_ngay = khoang_thoi_gian.days
        tong_giay = khoang_thoi_gian.seconds
        so_gio = tong_giay // 3600
        so_phut = (tong_giay % 3600) // 60
        so_giay = tong_giay % 60

        ket_qua_label.config(text=f"‚è≥ C√≤n l·∫°i: {so_ngay} ng√†y, {so_gio} gi·ªù, {so_phut} ph√∫t, {so_giay} gi√¢y.")

    except ValueError:
        ket_qua_label.config(text="üõë L·ªói ƒë·ªãnh d·∫°ng th·ªùi gian. H√£y ki·ªÉm tra l·∫°i.")

root = tk.Tk()
root.title("T√≠nh th·ªùi gian c√≤n l·∫°i")

tk.Label(root, text="Ch·ªçn ng√†y t∆∞∆°ng lai:").grid(row=0, column=0, pady=5)
calendar = DateEntry(root, date_pattern='yyyy-mm-dd')
calendar.grid(row=0, column=1, pady=5)

tk.Label(root, text="Gi·ªù (0-23):").grid(row=1, column=0)
gio_entry = ttk.Entry(root, width=5)
gio_entry.insert(0, "00")
gio_entry.grid(row=1, column=1)

tk.Label(root, text="Ph√∫t (0-59):").grid(row=2, column=0)
phut_entry = ttk.Entry(root, width=5)
phut_entry.insert(0, "00")
phut_entry.grid(row=2, column=1)

tk.Label(root, text="Gi√¢y (0-59):").grid(row=3, column=0)
giay_entry = ttk.Entry(root, width=5)
giay_entry.insert(0, "00")
giay_entry.grid(row=3, column=1)

tinh_button = ttk.Button(root, text="T√≠nh th·ªùi gian", command=tinh_thoi_gian_con_lai)
tinh_button.grid(row=4, column=0, columnspan=2, pady=10)

ket_qua_label = tk.Label(root, text="", fg="blue", font=('Arial', 12))
ket_qua_label.grid(row=5, column=0, columnspan=2, pady=10)

root.mainloop()






from pyspark.ml.evaluation import ClusteringEvaluator
# T·∫°o ClusteringEvaluator
evaluator = ClusteringEvaluator()

# T√≠nh Silhouette Score
silhouette = evaluator.evaluate(predictions_bkm)
print(f'Silhouette Score: {silhouette}')

"""###Gaussian"""

from pyspark.ml.clustering import GaussianMixture
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import functions as F

# X√≥a c·ªôt 'gmm_features' n·∫øu ƒë√£ t·ªìn t·∫°i
if 'gmm_features' in df_rfm.columns:
    df_rfm = df_rfm.drop('gmm_features')

# T·∫°o VectorAssembler cho c√°c ƒë·∫∑c tr∆∞ng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="gmm_features")
df_rfm = assembler.transform(df_rfm)

# X√¢y d·ª±ng m√¥ h√¨nh Gaussian Mixture
gmm = GaussianMixture(k=3, seed=1)
model = gmm.fit(df_rfm)

# D·ª± ƒëo√°n c·ª•m cho t·ª´ng kh√°ch h√†ng
predictions_gmm = model.transform(df_rfm)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
predictions_gmm.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Th√™m t√™n c·ª•m v√†o k·∫øt qu·∫£
def label_cluster(prediction):
    if prediction == 0:
        return "Kh√°ch h√†ng kh√¥ng ti·ªÅm nƒÉng"
    elif prediction == 1:
        return "Kh√°ch h√†ng ti·ªÅm nƒÉng"
    else:
        return "Kh√°ch h√†ng m·ªõi"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# T·∫°o UDF
label_udf = udf(label_cluster, StringType())

# √Åp d·ª•ng UDF ƒë·ªÉ th√™m c·ªôt 'customer_segment'
predictions_3 = predictions_gmm.withColumn("customer_segment", label_udf("prediction"))

# Hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë·ªÉ ki·ªÉm tra
predictions_3.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

print(predictions.columns)

# S·ª≠ d·ª•ng groupBy ƒë·ªÉ t√≠nh to√°n th·ªëng k√™ cho t·ª´ng c·ª•m
grouped_results_gmm = predictions_3.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
grouped_results_gmm.show()

# ƒê√°nh gi√° m√¥ h√¨nh (n·∫øu c·∫ßn)
# GMM kh√¥ng c√≥ ch·ªâ s·ªë Silhouette nh∆∞ K-Means, nh∆∞ng c√≥ th·ªÉ ki·ªÉm tra ƒë·ªô tin c·∫≠y c·ªßa m√¥ h√¨nh b·∫±ng c√°ch xem c√°c tham s·ªë c·ªßa n√≥
print("Gaussian Mixture Model Parameters:")
for i in range(model.getK()):
    print(f"Component {i}:")
    print(f"  Weights: {model.weights[i]}")
    print(f"  Mean: {model.gaussians[i].mean}")
    print(f"  Covariance: {model.gaussians[i].cov}")

"""#So s√°nh"""

# ƒê√°nh gi√° m√¥ h√¨nh K-Means
kmeans_silhouette = evaluator.evaluate(predictions_1)
print(f'Silhouette Score for K-Means: {kmeans_silhouette}')

# ƒê√°nh gi√° m√¥ h√¨nh Bisecting K-Means
bkm_silhouette = evaluator.evaluate(predictions_2)
print(f'Silhouette Score for Bisecting K-Means: {bkm_silhouette}')

# In th√¥ng s·ªë c·ªßa GMM
print("Gaussian Mixture Model Parameters:")
for i in range(model.getK()):
    print(f"Component {i}:")
    print(f"  Weights: {model.weights[i]}")
    print(f"  Mean: {model.gaussians[i].mean}")
    print(f"  Covariance: {model.gaussians[i].cov}")

"""####So s√°nh s·ªë l∆∞·ª£ng c·ª•m"""

grouped_results_km.show()

grouped_results_bkm.show()

grouped_results_gmm.show()

# K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ c√°c m√¥ h√¨nh d·ª±a tr√™n customer_id
combined_df = predictions_1.alias("km") \
    .join(predictions_2.alias("bkm"), "customer_id") \
    .join(predictions_3.alias("gmm"), "customer_id") \
    .select("customer_id",
            col("km.customer_segment").alias("km_segment"),
            col("bkm.customer_segment").alias("bkm_segment"),
            col("gmm.customer_segment").alias("gmm_segment"))

# Hi·ªÉn th·ªã d·ªØ li·ªáu k·∫øt h·ª£p
combined_df.show()

from pyspark.sql import functions as F

# K·∫øt h·ª£p c√°c d·ª± ƒëo√°n c·ªßa ba m√¥ h√¨nh
df_combined = predictions_1.alias('km') \
    .join(predictions_2.alias('bkm'), on="customer_id") \
    .join(predictions_3.alias('gmm'), on="customer_id") \
    .select(
        F.col('km.customer_id'),
        F.col('km.prediction').alias('km_prediction'),
        F.col('bkm.prediction').alias('bkm_prediction'),
        F.col('gmm.prediction').alias('gmm_prediction')
    )

# ƒê·∫øm s·ªë l∆∞·ª£ng kh√°ch h√†ng c√≥ d·ª± ƒëo√°n gi·ªëng nhau gi·ªØa c√°c m√¥ h√¨nh
total_customers = df_combined.count()

# ƒê·∫øm s·ªë l∆∞·ª£ng kh√°ch h√†ng c√≥ d·ª± ƒëo√°n gi·ªëng nhau gi·ªØa c√°c m√¥ h√¨nh
matching_km_bkm = df_combined.filter(
    (F.col('km_prediction') == F.col('bkm_prediction'))
).count()

matching_km_gmm = df_combined.filter(
    (F.col('km_prediction') == F.col('gmm_prediction'))
).count()

matching_bkm_gmm = df_combined.filter(
    (F.col('bkm_prediction') == F.col('gmm_prediction'))
).count()

# T√≠nh t·ª∑ l·ªá tr√πng kh·ªõp
ratio_km_bkm = (matching_km_bkm / total_customers) * 100
ratio_km_gmm = (matching_km_gmm / total_customers) * 100
ratio_bkm_gmm = (matching_bkm_gmm / total_customers) * 100

print(f'T·ª∑ l·ªá tr√πng kh·ªõp gi·ªØa K-Means v√† Bisecting K-Means: {ratio_km_bkm:.2f}%')
print(f'T·ª∑ l·ªá tr√πng kh·ªõp gi·ªØa K-Means v√† Gaussian Mixture Models: {ratio_km_gmm:.2f}%')
print(f'T·ª∑ l·ªá tr√πng kh·ªõp gi·ªØa Bisecting K-Means v√† Gaussian Mixture Models: {ratio_bkm_gmm:.2f}%')

#Tr·ª±c quan h√≥a k·∫øt qu·∫£

# Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t cho t·ª´ng c·ª•m K-Means
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='km_segment', palette='viridis')
plt.title('K-Means Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t cho t·ª´ng c·ª•m Bisecting K-Means
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='bkm_segment', palette='viridis')
plt.title('Bisecting K-Means Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t cho t·ª´ng c·ª•m Gaussian Mixture
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='gmm_segment', palette='viridis')
plt.title('Gaussian Mixture Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

