# -*- coding: utf-8 -*-
"""SIC_data preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DOrjRMPUnwbRg573GoT_mcGksE3WwCE

#Cài đặt Pyspark
"""

"""# Input thư viện và đọc dữ liệu"""

import pyspark
import pandas as pd
import numpy as np

from pyspark.sql import SparkSession

# Khởi tạo SparkSession với cấu hình HDFS
spark = SparkSession.builder \
    .appName("Customer Preprocessing") \
    .master("local[*]") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .config("spark.hadoop.fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem") \
    .getOrCreate()

# Đọc CSV từ HDFS
hdfs_path = "hdfs://localhost:9000/user/iamphuong/customer/customer_data1.csv"
df_spark = spark.read.csv(hdfs_path, header=True, inferSchema=True)

# Hiển thị một vài dòng dữ liệu
df_spark.show()

print(spark.version)

# Hiển thị dữ liệu
df_spark.show()

df_spark.printSchema()

df_spark.describe().show()

print(df_spark.columns)

print(df_spark.dtypes)

"""#Tiền xử lý dữ liệu"""

# Xóa tất cả các giá trị NULL
df_spark = df_spark.dropna()

#Loại bỏ cột không cần thiết cho việc phân tích và xử lý
df_spark = df_spark.drop("shopping_mall")

#Chuyển đổi kiểu dữ liệu
from pyspark.sql.functions import col

df_spark = df_spark.withColumn("age", col("age").cast("integer"))
df_spark = df_spark.withColumn("quantity", col("quantity").cast("integer"))
df_spark = df_spark.withColumn("price", col("price").cast("double"))

#Thay đổi định dạng ngày tháng
from pyspark.sql.functions import to_date

df_spark = df_spark.withColumn("invoice_date", to_date(col("invoice_date"), "yyyy-MM-dd"))

#Tạo thêm 1 trường thông tin mới "Tổng tiền của mỗi hóa đơn"
df_spark = df_spark.withColumn("total_amount", col("quantity") * col("price"))

#Xóa các giá trị trùng lặp
df_spark = df_spark.dropDuplicates()

#Mã hóa dữ liệu phân loại
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCols=["gender", "category", "payment_method"],
                        outputCols=["gender_indexed", "category_indexed", "payment_method_indexed"])
df_spark = indexer.fit(df_spark).transform(df_spark)

# Tạo cột "Tổng tiền của mỗi hóa đơn"
df_spark = df_spark.withColumn("total_amount", col("quantity") * col("price"))

#Chuẩn hóa dữ liệu số Scaling
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["age", "quantity", "price", "total_amount"],
                            outputCol="features")
df_spark = assembler.transform(df_spark)

scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
df_spark = scaler.fit(df_spark).transform(df_spark)

#Tính toán thống kê cơ bản
df_spark.describe().show()

# Xóa cột 'features' trong DataFrame
if 'features' in df_spark.columns:
    df_spark = df_spark.drop('features')

from pyspark.sql import SparkSession
from pyspark.sql.functions import datediff, max, count, sum, col, when, lit, expr
from pyspark.sql.window import Window

# Chuyển đổi invoice_date thành kiểu date
df_spark = df_spark.withColumn("invoice_date", col("invoice_date").cast("date"))

# Tính Recency, Frequency, và Monetary
max_date = df_spark.agg(max("invoice_date")).collect()[0][0]
df_rfm = df_spark.groupBy("customer_id").agg(
    datediff(lit(max_date), max("invoice_date")).alias("recency"),
    count("invoice_no").alias("frequency"),
    sum(col("price") * col("quantity")).alias("monetary")
)

from pyspark.ml.feature import VectorAssembler

# Tạo VectorAssembler để kết hợp các đặc trưng thành một vector
assembler = VectorAssembler(inputCols=["scaled_features"], outputCol="features")

# Chuyển đổi DataFrame với VectorAssembler
df_spark = assembler.transform(df_spark)

#Hiển thị dữ liệu
df_spark.show()

"""#Trực quan hóa dữ liệu"""

# Chuyển đổi sang Pandas DataFrame
df_pandas = df_spark.select("*").toPandas()

#Biều đồ phân bố độ tuổi của khách hàng
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df_pandas['age'], bins=30, kde=True)
plt.title('Age Distribution of Customers')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

#Biểu đồ tần suất của phương thức thanh toán

plt.figure(figsize=(10, 6))
sns.countplot(x='payment_method', data=df_pandas)
plt.title('Payment Method Frequency')
plt.xlabel('Payment Method')
plt.ylabel('Frequency')
plt.show()

#Biểu đồ tương quan giữa số lượng và tổng tiền mỗi hóa đơn

plt.figure(figsize=(10, 6))
sns.scatterplot(x='quantity', y='total_amount', data=df_pandas)
plt.title('Quantity vs Total Amount')
plt.xlabel('Quantity')
plt.ylabel('Total Amount')
plt.show()

"""#Phân chia tập dữ liệu"""

train_df, temp_df = df_spark.randomSplit([0.7, 0.3], seed=42)
validation_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)

"""#Xây dựng mô hình học máy

###K-Means
"""

from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.feature import VectorAssembler

# Tạo VectorAssembler cho các đặc trưng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="features")
df_rfm = assembler.transform(df_rfm)

# Xây dựng mô hình K-Means
kmeans = KMeans(k=3, seed=1)
model = kmeans.fit(df_rfm)

# Dự đoán cụm cho từng khách hàng
predictions_km = model.transform(df_rfm)

# Hiển thị kết quả
predictions_km.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Thêm tên cụm vào kết quả
def label_cluster(prediction):
    if prediction == 0:
        return "Khách hàng không tiềm năng"
    elif prediction == 1:
        return "Khách hàng tiềm năng"
    else:
        return "Khách hàng mới"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

label_udf = udf(label_cluster, StringType())
predictions_1 = predictions_km.withColumn("customer_segment", label_udf("prediction"))

# Hiển thị kết quả
predictions_1.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

from pyspark.sql import functions as F

# Dự đoán cụm cho từng khách hàng
predictions = model.transform(df_rfm)

# Thêm tên cụm vào kết quả
predictions = predictions_1.withColumn("customer_segment", label_udf("prediction"))

# Sử dụng groupBy để tính toán thống kê cho từng cụm
grouped_results_km = predictions.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hiển thị kết quả
grouped_results_km.show()

# Đánh giá mô hình
evaluator = ClusteringEvaluator()
silhouette = evaluator.evaluate(predictions)
print(f'Silhouette Score: {silhouette}')

"""###Bisecting Kmeans"""

from pyspark.sql.functions import col

# Xóa cột 'features' nếu nó đã tồn tại
if 'features' in df_rfm.columns:
    df_rfm = df_rfm.drop('features')

# Tạo VectorAssembler cho các đặc trưng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="features")
df_rfm = assembler.transform(df_rfm)

# Xây dựng mô hình BisectingKMeans
from pyspark.ml.clustering import BisectingKMeans

bkm = BisectingKMeans(k=3, seed=1)
model = bkm.fit(df_rfm)

# Dự đoán cụm cho từng khách hàng
predictions_bkm = model.transform(df_rfm)

# Hiển thị kết quả
predictions_bkm.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Thêm tên cụm vào kết quả
def label_cluster(prediction):
    if prediction == 0:
        return "Khách hàng không tiềm năng"
    elif prediction == 1:
        return "Khách hàng tiềm năng"
    else:
        return "Khách hàng mới"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

label_udf = udf(label_cluster, StringType())
predictions_2 = predictions_bkm.withColumn("customer_segment", label_udf("prediction"))

# Hiển thị kết quả
predictions_2.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

from pyspark.sql import functions as F

# Sử dụng groupBy để tính toán thống kê cho từng cụm
grouped_results_bkm = predictions_2.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hiển thị kết quả
grouped_results_bkm.show()

from pyspark.ml.evaluation import ClusteringEvaluator

# Tạo ClusteringEvaluator
evaluator = ClusteringEvaluator()

# Tính Silhouette Score
silhouette = evaluator.evaluate(predictions_bkm)
print(f'Silhouette Score: {silhouette}')

"""###Gaussian"""

from pyspark.ml.clustering import GaussianMixture
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import functions as F

# Xóa cột 'gmm_features' nếu đã tồn tại
if 'gmm_features' in df_rfm.columns:
    df_rfm = df_rfm.drop('gmm_features')

# Tạo VectorAssembler cho các đặc trưng recency, frequency, monetary
assembler = VectorAssembler(inputCols=["recency", "frequency", "monetary"], outputCol="gmm_features")
df_rfm = assembler.transform(df_rfm)

# Xây dựng mô hình Gaussian Mixture
gmm = GaussianMixture(k=3, seed=1)
model = gmm.fit(df_rfm)

# Dự đoán cụm cho từng khách hàng
predictions_gmm = model.transform(df_rfm)

# Hiển thị kết quả
predictions_gmm.select("customer_id", "recency", "frequency", "monetary", "prediction").show()

# Thêm tên cụm vào kết quả
def label_cluster(prediction):
    if prediction == 0:
        return "Khách hàng không tiềm năng"
    elif prediction == 1:
        return "Khách hàng tiềm năng"
    else:
        return "Khách hàng mới"

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Tạo UDF
label_udf = udf(label_cluster, StringType())

# Áp dụng UDF để thêm cột 'customer_segment'
predictions_3 = predictions_gmm.withColumn("customer_segment", label_udf("prediction"))

# Hiển thị kết quả để kiểm tra
predictions_3.select("customer_id", "recency", "frequency", "monetary", "prediction", "customer_segment").show()

print(predictions.columns)

# Sử dụng groupBy để tính toán thống kê cho từng cụm
grouped_results_gmm = predictions_3.groupBy("customer_segment").agg(
    F.count("customer_id").alias("count"),
    F.avg("recency").alias("avg_recency"),
    F.avg("frequency").alias("avg_frequency"),
    F.avg("monetary").alias("avg_monetary")
)

# Hiển thị kết quả
grouped_results_gmm.show()

# Đánh giá mô hình (nếu cần)
# GMM không có chỉ số Silhouette như K-Means, nhưng có thể kiểm tra độ tin cậy của mô hình bằng cách xem các tham số của nó
print("Gaussian Mixture Model Parameters:")
for i in range(model.getK()):
    print(f"Component {i}:")
    print(f"  Weights: {model.weights[i]}")
    print(f"  Mean: {model.gaussians[i].mean}")
    print(f"  Covariance: {model.gaussians[i].cov}")

"""#So sánh"""

# Đánh giá mô hình K-Means
kmeans_silhouette = evaluator.evaluate(predictions_1)
print(f'Silhouette Score for K-Means: {kmeans_silhouette}')

# Đánh giá mô hình Bisecting K-Means
bkm_silhouette = evaluator.evaluate(predictions_2)
print(f'Silhouette Score for Bisecting K-Means: {bkm_silhouette}')

# In thông số của GMM
print("Gaussian Mixture Model Parameters:")
for i in range(model.getK()):
    print(f"Component {i}:")
    print(f"  Weights: {model.weights[i]}")
    print(f"  Mean: {model.gaussians[i].mean}")
    print(f"  Covariance: {model.gaussians[i].cov}")

"""####So sánh số lượng cụm"""

grouped_results_km.show()

grouped_results_bkm.show()

grouped_results_gmm.show()

# Kết hợp kết quả từ các mô hình dựa trên customer_id
combined_df = predictions_1.alias("km") \
    .join(predictions_2.alias("bkm"), "customer_id") \
    .join(predictions_3.alias("gmm"), "customer_id") \
    .select("customer_id",
            col("km.customer_segment").alias("km_segment"),
            col("bkm.customer_segment").alias("bkm_segment"),
            col("gmm.customer_segment").alias("gmm_segment"))

# Hiển thị dữ liệu kết hợp
combined_df.show()

from pyspark.sql import functions as F

# Kết hợp các dự đoán của ba mô hình
df_combined = predictions_1.alias('km') \
    .join(predictions_2.alias('bkm'), on="customer_id") \
    .join(predictions_3.alias('gmm'), on="customer_id") \
    .select(
        F.col('km.customer_id'),
        F.col('km.prediction').alias('km_prediction'),
        F.col('bkm.prediction').alias('bkm_prediction'),
        F.col('gmm.prediction').alias('gmm_prediction')
    )

# Đếm số lượng khách hàng có dự đoán giống nhau giữa các mô hình
total_customers = df_combined.count()

# Đếm số lượng khách hàng có dự đoán giống nhau giữa các mô hình
matching_km_bkm = df_combined.filter(
    (F.col('km_prediction') == F.col('bkm_prediction'))
).count()

matching_km_gmm = df_combined.filter(
    (F.col('km_prediction') == F.col('gmm_prediction'))
).count()

matching_bkm_gmm = df_combined.filter(
    (F.col('bkm_prediction') == F.col('gmm_prediction'))
).count()

# Tính tỷ lệ trùng khớp
ratio_km_bkm = (matching_km_bkm / total_customers) * 100
ratio_km_gmm = (matching_km_gmm / total_customers) * 100
ratio_bkm_gmm = (matching_bkm_gmm / total_customers) * 100

print(f'Tỷ lệ trùng khớp giữa K-Means và Bisecting K-Means: {ratio_km_bkm:.2f}%')
print(f'Tỷ lệ trùng khớp giữa K-Means và Gaussian Mixture Models: {ratio_km_gmm:.2f}%')
print(f'Tỷ lệ trùng khớp giữa Bisecting K-Means và Gaussian Mixture Models: {ratio_bkm_gmm:.2f}%')

#Trực quan hóa kết quả

# Biểu đồ tần suất cho từng cụm K-Means
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='km_segment', palette='viridis')
plt.title('K-Means Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# Biểu đồ tần suất cho từng cụm Bisecting K-Means
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='bkm_segment', palette='viridis')
plt.title('Bisecting K-Means Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# Biểu đồ tần suất cho từng cụm Gaussian Mixture
plt.figure(figsize=(12, 6))
sns.countplot(data=combined_df.toPandas(), x='gmm_segment', palette='viridis')
plt.title('Gaussian Mixture Customer Segments Frequency')
plt.xlabel('Customer Segment')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

